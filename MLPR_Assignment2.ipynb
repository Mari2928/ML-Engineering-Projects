{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import *\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('ct_data.npz')\n",
    "X_train = data['X_train']; X_val = data['X_val']; X_test = data['X_test']\n",
    "y_train = data['y_train']; y_val = data['y_val']; y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_and_errorbar(data):\n",
    "    N = data.shape[0]\n",
    "    sample_mean = 1/N * np.sum(data)\n",
    "    sample_var = 1/(N-1) * np.sum((data - sample_mean)**2)\n",
    "    sample_std = np.sqrt(sample_var)\n",
    "    standard_error = sample_std / np.sqrt(N)\n",
    "    return sample_mean, standard_error\n",
    "\n",
    "sample_mean_y_val, standard_error_y_val = get_mean_and_errorbar(y_val)\n",
    "\n",
    "partial_y_train = y_train[:5785]\n",
    "sample_mean_y_train, standard_error_y_train = get_mean_and_errorbar(partial_y_train)\n",
    "\n",
    "txt1 = \"y_val mean with a standard error: {mean:.10f} +/- {stderr:.10f}\"\n",
    "print(txt1.format(mean = sample_mean_y_val, stderr = standard_error_y_val))\n",
    "\n",
    "txt2 = \"partial_y_train mean with a standard error: {mean:.10f} +/- {stderr:.10f}\"\n",
    "print(txt2.format(mean = sample_mean_y_train, \n",
    "                 stderr = standard_error_y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the different sample means from y_val and partial_y_train datasets. The results demonstrate that we can't rely on the standard error bars because the sample means are random variables, and the errors are computed based on them, not the true mean. The standard error bars are misleading because one might rely on the indication to see the average locations in future CT slices. They indicate how precisely we have measured the mean of the distribution with our N samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_constants = []\n",
    "remove_duplicates = []\n",
    "unique = []\n",
    "for col in range(X_train.shape[1]):  \n",
    "    # remove constants\n",
    "    if np.all(X_train[:,col]==X_train[:,col][0]):\n",
    "        remove_constants.append(col) \n",
    "        continue\n",
    "    # remove duplicates\n",
    "    if list(X_train[:,col]) not in unique:\n",
    "        unique.append(list(X_train[:,col]))        \n",
    "    else:\n",
    "        remove_duplicates.append(col)  \n",
    "\n",
    "remove_both = remove_constants + remove_duplicates\n",
    "X_train = np.delete(X_train, remove_both, axis=1)\n",
    "X_val = np.delete(X_val, remove_both, axis=1)\n",
    "X_test = np.delete(X_test, remove_both, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40754, 373)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59, 69, 179, 189, 351]"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[78, 79, 188, 199, 287, 359]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(X_train)\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(X_all_dup_removed)\n",
    "df2.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear regression baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared error on training set: 0.3567565397204054\n",
      "Root mean squared error on validation set: 0.4230521968394694\n"
     ]
    }
   ],
   "source": [
    "def fit_linreg(X, yy, alpha):\n",
    "    K = X.shape[1] + 1\n",
    "    design_matrix = np.c_[X, np.ones(X.shape[0])] # add a column of ones for the bias\n",
    "    alpha_identity = np.sqrt(alpha) * np.identity(K)\n",
    "    design_matrix = np.vstack((design_matrix, alpha_identity)) # add regularizer\n",
    "    \n",
    "    design_matrix[design_matrix.shape[0]-1][K-1] = 0 # remove penalty for the bias\n",
    "    augmented_y= np.hstack([yy, np.zeros(K)]) # augment vector of observations\n",
    "    \n",
    "    fit = np.linalg.lstsq(design_matrix, augmented_y, rcond = None)\n",
    "    weights = fit[0][:-1]\n",
    "    bias = fit[0][-1]\n",
    "    return weights, bias\n",
    "\n",
    "    \n",
    "def rmse(true, pred):\n",
    "    RSME = np.sqrt(np.square(np.subtract(true, pred)).mean())\n",
    "    return RSME\n",
    "\n",
    "alpha = 30\n",
    "weights, bias = fit_linreg(X_train, y_train, alpha)\n",
    "y_pred_train = np.dot(X_train, weights) + bias\n",
    "y_pred_val = np.dot(X_val, weights) + bias\n",
    "train_error = rmse(y_train, y_pred_train)\n",
    "val_error = rmse(y_val, y_pred_val)\n",
    "\n",
    "print('Root mean squared error on training set: {}'.format(train_error))\n",
    "print('Root mean squared error on validation set: {}'.format(val_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared error on training set: 0.35675625907436664\n",
      "Root mean squared error on validation set: 0.42305021197807124\n"
     ]
    }
   ],
   "source": [
    "ww_gradopt, bb_gradopt = fit_linreg_gradopt(X_train, y_train, alpha)\n",
    "y_pred_train = np.dot(X_train, ww_gradopt) + bb_gradopt\n",
    "y_pred_val = np.dot(X_val, ww_gradopt) + bb_gradopt\n",
    "train_errs = rmse(y_train, y_pred_train)\n",
    "val_errs = rmse(y_val, y_pred_val)\n",
    "print('Root mean squared error on training set: {}'.format(train_errs))\n",
    "print('Root mean squared error on validation set: {}'.format(val_errs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are weights by lstsq and solver the same?: False\n"
     ]
    }
   ],
   "source": [
    "K = X_train.shape[1]\n",
    "X_train_reg = np.vstack([X_train, np.sqrt(alpha) * np.identity(K)])\n",
    "y_train_reg = np.hstack([y_train, np.zeros(K)])  \n",
    "\n",
    "ww_solver = np.linalg.solve(np.dot(X_train_reg.T, X_train_reg), \n",
    "                            np.dot(X_train_reg.T, y_train_reg))\n",
    "print('Are weights by lstsq and solver the same?: {}'\n",
    "      .format(np.array_equal(np.round(weights, 5), np.round(ww_solver, 5))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the almost same root mean squared errors from our fit_linreg and the provided fit_linreg_gradopt. It is because (X.T * X) is invertible, and the number of samples is greater than or equal to the number of dimensions (N >= D), so the normal-equation is defined, and there is a unique solution for the best setting of the weights. In such a case, the gradient-based optimizer finds the same answer because the cost function is strictly convex and has one unique minimum. We can also confirm from the below sanity check that the normal-equation gives the same weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Invented classification tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logreg_gradopt(X, yy, alpha):\n",
    "    D = X.shape[1]\n",
    "    args = (X, yy, alpha)\n",
    "    #init = (np.zeros(D), np.array(0))\n",
    "    init = (0.01 * np.random.randn(D) / np.sqrt(D), np.array(0))\n",
    "    ww, bb = minimize_list(logreg_cost, init, args)\n",
    "    return ww, bb\n",
    "\n",
    "def sigmoid(a): return 1. / (1. + np.exp(-a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40754, 20)\n"
     ]
    }
   ],
   "source": [
    "alpha = 30\n",
    "V = []\n",
    "bk = []\n",
    "\n",
    "K = 20 # number of thresholded classification problems to fit\n",
    "mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)\n",
    "for kk in range(K):\n",
    "    labels = y_train > thresholds[kk]\n",
    "    # fit logistic regression to these labels\n",
    "    ww, bb = fit_logreg_gradopt(X_train, labels, alpha)\n",
    "    V.append(ww)\n",
    "    bk.append(bb)\n",
    "\n",
    "V = np.array(V)\n",
    "bk = np.array(bk)\n",
    "\n",
    "X_train_prob = sigmoid(np.dot(X_train, V.T) + bk)\n",
    "X_val_prob = sigmoid(np.dot(X_val, V.T) + bk)\n",
    "X_test_prob = sigmoid(np.dot(X_test, V.T) + bk)\n",
    "\n",
    "print(X_train_prob.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root mean squared error on training set: 0.1544105592251483\n",
      "Root mean squared error on validation set: 0.25424931879375523\n"
     ]
    }
   ],
   "source": [
    "# fit regularized linear regression to the transformed datasets\n",
    "ww, bb = fit_linreg(X_train_prob, y_train, alpha)\n",
    "y_pred_train = np.dot(X_train_prob, ww) + bb\n",
    "y_pred_val = np.dot(X_val_prob, ww) + bb\n",
    "train_errs = rmse(y_train, y_pred_train)\n",
    "val_errs = rmse(y_val, y_pred_val)\n",
    "print('Root mean squared error on training set: {}'.format(train_errs))\n",
    "print('Root mean squared error on validation set: {}'.format(val_errs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Small neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_a(D, K=1):\n",
    "    np.random.seed(19)\n",
    "    # Glorot and Bengio's init\n",
    "    if K > 1:\n",
    "        std = (2. / (K + D))**0.5\n",
    "    else:\n",
    "        std = (1. / K)**0.5\n",
    "    ww = np.random.normal(loc=0., scale=std, size=K)\n",
    "    bb = np.array(0)\n",
    "    V = np.random.normal(loc=0., scale=std, size=(K, D))\n",
    "    bk = std * np.random.randn(K) / np.sqrt(K)\n",
    "    return (ww, bb, V, bk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_b(D, K=1):    \n",
    "    W_matrix_log\n",
    "    bias_vector_log \n",
    "\n",
    "    ww = np.zeros(K)\n",
    "    bb = np.array(0)\n",
    "    V = np.zeros((K, D))\n",
    "    bk = np.zeros(K)\n",
    "    return (ww, bb, V, bk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn(X_train, X_val, init, K, alpha=30):\n",
    "    args = (X_train, y_train, alpha) \n",
    "    ww, bb, V, bk = minimize_list(nn_cost, init(X_train.shape[1], K), args)\n",
    "    params = (ww, bb, V, bk)\n",
    "    y_pred_train = nn_cost(params, X_train)\n",
    "    y_pred_val = nn_cost(params, X_val)\n",
    "    train_errs = rmse(y_train, y_pred_train)\n",
    "    val_errs = rmse(y_val, y_pred_val)\n",
    "    return train_errs, val_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init_a: RMSE on training set: 0.13983192980941112\n",
      "Init_a: RMSE on validation set: 0.2711542448010652\n"
     ]
    }
   ],
   "source": [
    "# fit NN with a sensible random initialization\n",
    "train_errs, val_errs = train_nn(X_train, X_val, init_a, K=20)\n",
    "print('Init_a: RMSE on training set: {}'.format(train_errs))\n",
    "print('Init_a: RMSE on validation set: {}'.format(val_errs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on training set using prefitted weights: 0.1387897525445384\n",
      "RMSE on validation set using prefitted weights: 0.27079711742451973\n"
     ]
    }
   ],
   "source": [
    "# fit NN with the parameters initialized using the fits made in Q3\n",
    "args = (X_train, y_train, alpha) \n",
    "ww, bb, V, bk = minimize_list(nn_cost, (ww, bb, V, bk), args)\n",
    "params = (ww, bb, V, bk)\n",
    "y_pred_train = nn_cost(params, X_train)\n",
    "y_pred_val = nn_cost(params, X_val)\n",
    "train_errs = rmse(y_train, y_pred_train)\n",
    "val_errs = rmse(y_val, y_pred_val)\n",
    "print('Init_a: RMSE on training set: {}'.format(train_errs))\n",
    "print('Init_a: RMSE on validation set: {}'.format(val_errs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The strategy (a) that initializes the weights randomly works better than the one (b) used in Q3, which sets all the weights to zero. In (b), unlike (a), our neural network function only depends on a single linear combination of the inputs (X_train_prob) because our K hidden units start with the same parameters so that each unit gets the same gradient vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args = (X_train, y_train, alpha) \n",
    "minimize_list(nn_cost, init_b(X_train.shape[1], K=20), args)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.05383299e-01,  6.01259784e-03, -3.42595177e-02, ...,\n",
       "         1.52258932e-01, -2.11320448e-02,  1.34620981e-02],\n",
       "       [ 6.41236638e-03,  1.01879122e-02,  2.05937008e-03, ...,\n",
       "         1.12573064e-03,  1.83921041e-03, -4.90584301e-04],\n",
       "       [ 3.40601229e-02,  3.19521382e-02,  5.00998874e-03, ...,\n",
       "        -6.46171803e-03,  4.30461944e-03,  8.46586084e-04],\n",
       "       ...,\n",
       "       [ 6.72947646e-03,  1.14041061e-02,  2.30239212e-03, ...,\n",
       "         1.65891885e-03,  2.22419736e-03, -6.06355079e-04],\n",
       "       [ 3.37371538e-03,  4.88399052e-03,  9.24689633e-04, ...,\n",
       "         3.82506420e-06,  5.51003745e-04,  1.18439567e-05],\n",
       "       [ 6.98182734e-03,  1.14713770e-02,  2.31584395e-03, ...,\n",
       "         1.61313213e-03,  2.16697232e-03, -5.95725651e-04]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize_list(nn_cost, init_a(X_train.shape[1], K=20), args)[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jointly fitting the neural network works better because the Q3 model only fits weights for a single linear combination as a feedforward neural network. On the other hand, the neural network model fits parameters for one hidden layer with K=20 hidden units using a gradient-based optimizer, reducing the cost by updating the parameters. Although the cost function is not convex, this optimization procedure using forward and back propagations likely to find the local optima with a better cost and generalized better on the validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_nn_reg(alpha):\n",
    "#     train_errs, val_errs = train_nn(X_train_prob, X_val_prob, init_a, alpha=alpha)\n",
    "#     return val_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_errs = 0.26705441493687515 # Q4 validation error\n",
    "\n",
    "# X_rest = np.arange(0, 50, 0.02)\n",
    "# X_obs = np.random.choice(alphas, size=3)\n",
    "# idx = [np.where(alphas==x)[0][0] for x in X_obs]\n",
    "\n",
    "# yy = np.array([np.log(val_errs) - np.log(train_nn_reg(alpha)) for alpha in X_obs])\n",
    "# rest_cond_mu, rest_cond_cov = helpers.gp_post_par(X_rest, X_obs, yy)\n",
    "# PI_alpha = np.array([norm.cdf((rest_cond_mu[i] - np.max(yy))/np.sqrt(rest_cond_cov[i][i])) for i in idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5 iterations of probability of improvement acquisition function\n",
    "# alphas_valerrs = []\n",
    "# for i in range(5):     \n",
    "#     rmses = [[train_nn_reg(alpha), alpha] for alpha in PI_alpha]\n",
    "#     alphas_valerrs.append(rmses)\n",
    "#     yy = np.array([np.log(val_errs) - np.log(rmse[0]) for rmse in rmses])\n",
    "#     rest_cond_mu, rest_cond_cov = helpers.gp_post_par(X_rest, PI_alpha, yy)\n",
    "#     idx = [np.where(alphas == np.round(alpha, 1)) for alpha in PI_alpha]\n",
    "#     PI_alpha = np.array([norm.cdf((rest_cond_mu[i[0][0]] - np.max(yy))/\n",
    "#                                   np.sqrt(rest_cond_cov[i[0][0]][i[0][0]])) for i in idx])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best PI and its alpha\n",
      "1 0.26299832114162164 0.4764329186620903\n",
      "2 0.26300938166190846 0.4819881472031914\n",
      "3 0.26289831177491924 0.4744821046215118\n",
      "4 0.26301297585091915 0.4783196769420628\n",
      "5 0.2629760501084887 0.48150241928503074\n"
     ]
    }
   ],
   "source": [
    "# print(\"Best PI and its alpha\")\n",
    "# i = 1\n",
    "# best_PI_alpha = None\n",
    "# best_PI_alphas = []\n",
    "# for alpha_valerr in np.array(alphas_valerrs):\n",
    "#     best_PI = np.min(alpha_valerr)\n",
    "#     best_PI_alpha = alpha_valerr[np.where(alpha_valerr[:,0] == best_PI)]\n",
    "#     best_PI_alphas.append(best_PI_alpha)\n",
    "#     print(i, best_PI_alpha[0][0], best_PI_alpha[0][1])\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 0.4744821046215118\n"
     ]
    }
   ],
   "source": [
    "# best_PI_alphas = np.array(best_PI_alphas)\n",
    "# best_PI = np.min(best_PI_alphas)\n",
    "# best_alpha = best_PI_alphas[np.where(best_PI_alphas[:,0] == best_PI)][0][1]\n",
    "# print(\"Best alpha:\", best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on validation set: 0.2630035421721344\n",
      "RMSE on test set: 0.27425911987829765\n"
     ]
    }
   ],
   "source": [
    "# args = (X_train_prob, y_train, best_alpha) \n",
    "# ww, bb, V, bk = helpers.minimize_list(helpers.nn_cost, init_a(X_train_prob), args)\n",
    "# params = (ww, bb, V, bk)\n",
    "# y_pred_val = helpers.nn_cost(params, X_val_prob)\n",
    "# y_pred_test = helpers.nn_cost(params, X_test_prob)\n",
    "# val_errs = rmse(y_val, y_pred_val)\n",
    "# test_errs = rmse(y_test, y_pred_test)\n",
    "# print('RMSE on validation set: {}'.format(val_errs))\n",
    "# print('RMSE on test set: {}'.format(test_errs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian optimization2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "def train_nn_reg(alpha):\n",
    "    train_errs, val_errs = train_nn(X_train, X_val, init_a, K=20, alpha=alpha)\n",
    "    return val_errs\n",
    "\n",
    "# def train_nn_reg(X_train, y_train, test_input, test_output, alpha): \n",
    "    \n",
    "#     K = 20\n",
    "#     D = X_train.shape[1]\n",
    "    \n",
    "#     # initialize weights and args\n",
    "#     ww_init = glorot_init_normal((K,))\n",
    "#     W_init = glorot_init_normal((K,D))\n",
    "#     bb_init = 0\n",
    "#     B_init = np.zeros((K,))\n",
    "\n",
    "#     args = (X_train, y_train, alpha)\n",
    "#     init = (ww_init, bb_init, W_init, B_init)\n",
    "\n",
    "#     # get fitted weights and biases\n",
    "#     ww, bb, W, B = minimize_list(nn_cost, init, args)\n",
    "#     params = (ww, bb, W, B)\n",
    "    \n",
    "#     y_pred = nn_cost(params, test_input, None, alpha)\n",
    "#     error = rmse(test_output, y_pred)\n",
    "    \n",
    "#     return error\n",
    "    \n",
    "\n",
    "def prob_improvement(mean_alpha, std_alpha, yy):\n",
    "    max_y = max(yy)\n",
    "    pi_alpha = norm.cdf((mean_alpha - max_y) / std_alpha)\n",
    "    return pi_alpha\n",
    "\n",
    "\n",
    "def maximise_acquisition_function(alpha_rest, alpha_obs, y_obs):\n",
    "    # get GP posterior evaluated at test points\n",
    "    mean_rest, cov_rest = gp_post_par(alpha_rest, alpha_obs, y_obs)\n",
    "    std_rest = np.sqrt(np.diag(cov_rest))\n",
    "\n",
    "    # initialize values\n",
    "    alpha_best = 0\n",
    "    max_pi = 0\n",
    "    \n",
    "    # find alpha that yields the best PI\n",
    "    for alpha, mean, std in zip(alpha_rest, mean_rest, std_rest):\n",
    "        pi_alpha = prob_improvement(mean, std, y_obs)\n",
    "        \n",
    "        if pi_alpha > max_pi:\n",
    "            alpha_best = alpha\n",
    "            max_pi = pi_alpha\n",
    "            \n",
    "    return alpha_best, max_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# validation RMSE from Q4 using random initialization\n",
    "rmse_baseline = 0.2708803021948929\n",
    "\n",
    "# get three observations from training the neural network at three alpha values\n",
    "alpha_obs = np.array([10, 25, 40])\n",
    "\n",
    "rmse_val = []\n",
    "for a in alpha_obs:\n",
    "    val_err = train_nn_reg(a)\n",
    "    rmse_val.append(val_err)\n",
    "    \n",
    "# subtract your alpha-observed log RMSEs from the log of this baseline and take the resulting values as y^(1), ...,  y^(3)\n",
    "y_obs = np.log(rmse_baseline) - np.log(rmse_val)\n",
    "\n",
    "alpha_rest = np.arange(0, 50, 0.02)\n",
    "\n",
    "# remove already observed values of alpha\n",
    "alpha_rest = np.setdiff1d(alpha_rest, alpha_obs)\n",
    "\n",
    "results = []\n",
    "for i in range(5):\n",
    "    # find alpha with highest PI\n",
    "    alpha_best, max_pi = maximise_acquisition_function(alpha_rest, alpha_obs, y_obs)\n",
    "    \n",
    "    # add to observed locations and remove from test locations\n",
    "    alpha_obs = np.append(alpha_obs, alpha_best)\n",
    "    alpha_rest = np.setdiff1d(alpha_rest, alpha_obs)\n",
    "    \n",
    "    # train using this alpha\n",
    "    val_rmse_alpha = train_nn_reg(alpha_best)\n",
    "    y_alpha = np.log(rmse_baseline) - np.log(val_rmse_alpha)\n",
    "    \n",
    "    # add to observed values\n",
    "    y_obs = np.append(y_obs, y_alpha)\n",
    "    \n",
    "    results.append((alpha_best, max_pi, val_rmse_alpha))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION: 1, ALPHA: 10.620000000000001, PI: 0.44039086203373706, VAL_RMSE: 0.2627446293192474\n",
      "ITERATION: 2, ALPHA: 8.72, PI: 0.44338366078962627, VAL_RMSE: 0.25830412503646893\n",
      "ITERATION: 3, ALPHA: 6.38, PI: 0.40677517263838653, VAL_RMSE: 0.25577169609575706\n",
      "ITERATION: 4, ALPHA: 5.3, PI: 0.4062555770452248, VAL_RMSE: 0.2575502217047426\n",
      "ITERATION: 5, ALPHA: 5.32, PI: 0.39441565302052106, VAL_RMSE: 0.2560261364785499\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(results)):\n",
    "    result = results[i]\n",
    "    print(\"ITERATION: {}, ALPHA: {}, PI: {}, VAL_RMSE: {}\".format(i+1, result[0], result[1], result[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: 6.38\n",
      "Best RMSE on validation set: 0.25577169609575706\n",
      "Best RMSE on test set: 0.28963171053159736\n"
     ]
    }
   ],
   "source": [
    "# find best alpha and report its errors\n",
    "best_alpha = results[2][0]\n",
    "args = (X_train, y_train, best_alpha) \n",
    "ww, bb, V, bk = minimize_list(nn_cost, init_a(X_train.shape[1], K=20), args)\n",
    "params = (ww, bb, V, bk)\n",
    "y_pred_test = nn_cost(params, X_test)\n",
    "test_errs = rmse(y_test, y_pred_test)\n",
    "print('Best alpha: {}'.format(best_alpha))\n",
    "print('Best RMSE on validation set: {}'.format(results[2][2]))\n",
    "print('Best RMSE on test set: {}'.format(test_errs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have improved the validation RMSE from the Q4 model by 0.0151. The observation noise is coming from the random initialization of weights in the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION 6\n",
    "\n",
    "def one_hot_encode(labels, K):\n",
    "    # labels: target outputs\n",
    "    # K: number of classes\n",
    "    N = labels.shape[0]\n",
    "    \n",
    "    mx = np.max(labels); mn = np.min(labels); hh = (mx-mn)/K\n",
    "    #thresholds = np.arange(mn+hh, mx, hh)\n",
    "    thresholds = np.linspace(mn+hh, mx-hh, num=K-1, endpoint=True)\n",
    "    # find which class each y-value belongs to\n",
    "    y_class = np.searchsorted(thresholds, labels)\n",
    "    encoding = np.zeros((N, K))\n",
    "    # set corresponding class to 1\n",
    "    encoding[np.arange(N), y_class] = 1\n",
    "    \n",
    "    return encoding, y_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    # subtract the max from the exponent for numerical stability\n",
    "    exp_z = np.exp(z - z.max(-1)[:, None])\n",
    "    \n",
    "    # normalise as probabilities\n",
    "    prob = exp_z /exp_z.sum(-1)[:, None]\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_glorot(D, K):\n",
    "    np.random.seed(19)\n",
    "    # Glorot and Bengio's init\n",
    "    if K > 1:\n",
    "        std = (2. / (K + D))**0.5\n",
    "    else:\n",
    "        std = (1. / K)**0.5\n",
    "\n",
    "    ww = np.random.normal(loc=0., scale=std, size=(K, D))\n",
    "    bb = np.zeros((K,))\n",
    "    return (ww, bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_reg_cost(params, X, yy=None, alpha=None):\n",
    "\n",
    "    # Unpack parameters from list\n",
    "    W, b = params\n",
    "    \n",
    "    # FORWARD COMPUTATION\n",
    "    Z = np.dot(X, W.T) + b[None,:] # N,K\n",
    "    F = softmax(Z)\n",
    "    \n",
    "    # at test time\n",
    "    if yy is None:\n",
    "        # user wants prediction rather than training signal:\n",
    "        return F\n",
    "    \n",
    "    # compute cross-entropy error\n",
    "    # -np.mean(np.sum(targets * np.log(probs), axis=1))\n",
    "    E = -np.mean(np.sum(yy * np.log(F), axis=1)) + alpha*np.sum(W*W)\n",
    "    #print('Error:',E) \n",
    "\n",
    "    \n",
    "    # BACKPROPOGATION\n",
    "    \n",
    "    # derivate of E with respect to h2\n",
    "    # (probs - targets) / outputs.shape[0]\n",
    "    \n",
    "#     probs = np.exp(F - F.max(-1)[:, None])\n",
    "#     probs /= probs.sum(-1)[:, None]\n",
    "#     F_bar = (probs - yy) / F.shape[0]\n",
    "    F_bar = (F - yy) / yy.shape[0]\n",
    "    \n",
    "    # derivate of E with respect to z2\n",
    "    \n",
    "    # softmax\n",
    "    # outputs * (grads_wrt_outputs - (grads_wrt_outputs * outputs).sum(-1)[:, None]))\n",
    "    Z_bar = F * (F_bar - (F_bar * F).sum(-1)[:, None])\n",
    "    \n",
    "    # np.dot(grads_wrt_outputs.T, inputs)               \n",
    "    W_bar = np.dot(Z_bar.T, X) + 2*alpha*W # 2*alpha*W is the gradient of the penalty term\n",
    "    \n",
    "    # np.sum(grads_wrt_outputs, axis=0)\n",
    "    b_bar = np.sum(Z_bar, axis=0) # scalar\n",
    "\n",
    "    return E, (W_bar, b_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_softmax_units(K):\n",
    "    alpha=0.0001\n",
    "    D = X_train.shape[1]   \n",
    "    # pre-process labels\n",
    "    y_encoded, _ = one_hot_encode(y_train, K)\n",
    "\n",
    "    args = (X_train, y_encoded, alpha)\n",
    "    W, b = minimize_list(softmax_reg_cost, init_glorot(D, K), args)\n",
    "    params = (W, b)\n",
    "    X_train_trans = softmax_reg_cost(params, X_train, None, alpha)\n",
    "    X_val_trans = softmax_reg_cost(params, X_val, None, alpha)\n",
    "    X_test_trans = softmax_reg_cost(params, X_test, None, alpha)\n",
    "    \n",
    "    return X_train_trans, X_val_trans, X_test_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sigmoid_units(K, alpha):\n",
    "    X_train_ = []; X_val_ = []; X_test_ = []   \n",
    "\n",
    "    mx = np.max(y_train); mn = np.min(y_train); hh = (mx-mn)/(K+1)\n",
    "    thresholds = np.linspace(mn+hh, mx-hh, num=K, endpoint=True)\n",
    "    for kk in range(K):\n",
    "        labels = y_train > thresholds[kk]\n",
    "        # fit logistic regression to these labels\n",
    "        ww, bb = fit_logreg_gradopt(X_train, labels, alpha)\n",
    "        X_train_.append(sigmoid(np.dot(X_train, ww.T) + bb))\n",
    "        X_val_.append(sigmoid(np.dot(X_val, ww.T) + bb))\n",
    "        X_test_.append(sigmoid(np.dot(X_test, ww.T) + bb))\n",
    "    \n",
    "    return np.array(X_train_).T, np.array(X_val_).T, np.array(X_test_).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linreg(X_train, X_val, alpha=30):\n",
    "    ww, bb = fit_linreg(X_train, y_train, alpha)\n",
    "    y_pred_train = np.dot(X_train, ww) + bb\n",
    "    y_pred_val = np.dot(X_val, ww) + bb\n",
    "    train_error = rmse(y_train, y_pred_train)\n",
    "    val_error = rmse(y_val, y_pred_val)\n",
    "    return train_error, val_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21157779795386927"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_trans, X_val_trans,_ = make_softmax_units(32)\n",
    "train_errs, val_errs = train_small_nn(X_train_trans, X_val_trans, alpha=30)\n",
    "val_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal\n",
    "Number of classes: 20\n",
    "Training error: 0.2409605615667869\n",
    "Validation error: 0.2503275903968519\n",
    "\n",
    "# unif\n",
    "Number of classes: 20\n",
    "Training error: 0.2482897062704253\n",
    "Validation error: 0.25643079776478994"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid units RMSEs on validation set: [0.256, 0.254, 0.255, 0.255, 0.255]\n",
      "Softmax units RMSEs on validation set: [0.249, 0.272, 0.24, 0.245, 0.213]\n"
     ]
    }
   ],
   "source": [
    "ks = [10, 20, 30, 40, 50]\n",
    "alpha = 30\n",
    "errs_sig = []\n",
    "errs_sof = []\n",
    "\n",
    "for K in ks:\n",
    "    X_train_sig, X_val_sig,_ = make_sigmoid_units(K, alpha)\n",
    "    train_errs_sig, val_errs_sig = train_small_nn(X_train_sig, X_val_sig, alpha)\n",
    "    errs_sig.append(np.round(val_errs_sig, 3))\n",
    "    \n",
    "    X_train_sof, X_val_sof,_ = make_softmax_units(K)\n",
    "    train_errs_sof, val_errs_sof = train_small_nn(X_train_sof, X_val_sof, alpha)\n",
    "    errs_sof.append(np.round(val_errs_sof, 3))\n",
    "    \n",
    "print('Sigmoid units RMSEs on validation set:', errs_sig)\n",
    "print('Softmax units RMSEs on validation set:', errs_sof)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid units RMSEs on validation set: [0.249, 0.272, 0.24, 0.245, 0.213, 0.225, 0.233, 0.234]\n",
      "Softmax units RMSEs on validation set: [0.249, 0.272, 0.238, 0.239, 0.209, 0.218, 0.222, 0.222]\n"
     ]
    }
   ],
   "source": [
    "ks = [10, 20, 30, 40, 50, 60, 70, 80]\n",
    "alpha = 30\n",
    "errs_sof_linreg = []\n",
    "errs_sof_nn = []\n",
    "for K in ks:    \n",
    "    X_train_sof_linreg, X_val_sof_linreg,_ = make_softmax_units(K)\n",
    "    train_errs_sof_linreg, val_errs_sof_linreg = train_small_nn(X_train_sof_linreg, X_val_sof_linreg, alpha)\n",
    "    errs_sof_linreg.append(np.round(val_errs_sof_linreg, 3))\n",
    "    \n",
    "    X_train_sof_nn, X_val_sof_nn,_ = make_softmax_units(K)\n",
    "    train_errs_sof_nn, val_errs_sof_nn = train_nn(X_train_sof_nn, X_val_sof_nn, init_a, K, alpha)\n",
    "    errs_sof_nn.append(np.round(val_errs_sof_nn, 3))    \n",
    "    \n",
    "print('Softmax + fit_linreg RMSEs on validation set:', errs_sof_linreg)\n",
    "print('Softmax + nn_cost RMSEs on validation set:', errs_sof_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best RMSE on validation set: 0.22013729663098597\n",
      "Best RMSE on test set: 0.22111682986948125\n"
     ]
    }
   ],
   "source": [
    "# evaluate the best K on test set\n",
    "best_K = 49\n",
    "alpha = 30\n",
    "X_train_sof_nn, X_val_sof_nn, X_test_sof_nn = make_softmax_units(best_K)\n",
    "args = (X_train_sof_nn, y_train, alpha) \n",
    "ww, bb, V, bk = minimize_list(nn_cost, init_a(X_train_sof_nn.shape[1], best_K), args)\n",
    "params = (ww, bb, V, bk)\n",
    "y_pred_val = nn_cost(params, X_val_sof_nn)\n",
    "y_pred_test = nn_cost(params, X_test_sof_nn)\n",
    "val_error = rmse(y_val, y_pred_val)\n",
    "test_error = rmse(y_test, y_pred_test)\n",
    "print('Best RMSE on validation set: {}'.format(val_error))\n",
    "print('Best RMSE on test set: {}'.format(test_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([20, 40, 60, 80])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(20, 100, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid units RMSEs on validation set: [0.254, 0.255, 0.254, 0.255]\n",
      "Softmax units RMSEs on validation set: [0.272, 0.245, 0.225, 0.234]\n"
     ]
    }
   ],
   "source": [
    "# compare softmax with sigmoid models\n",
    "ks = np.arange(20, 100, 20)\n",
    "alpha = 30\n",
    "errs_sig = []\n",
    "errs_sof = []\n",
    "\n",
    "for K in ks:\n",
    "    X_train_sig, X_val_sig,_ = make_sigmoid_units(K, alpha)\n",
    "    train_errs_sig, val_errs_sig = train_linreg(X_train_sig, X_val_sig, alpha)\n",
    "    errs_sig.append((K, val_errs_sig))\n",
    "    \n",
    "    X_train_sof, X_val_sof,_ = make_softmax_units(K)\n",
    "    train_errs_sof, val_errs_sof = train_linreg(X_train_sof, X_val_sof, alpha)\n",
    "    errs_sof.append((K, val_errs_sof))\n",
    "    \n",
    "    \n",
    "errs_sig_print = [np.round(result[1], 3) for result in errs_sig]\n",
    "errs_sof_print = [np.round(result[1], 3) for result in errs_sof]\n",
    "    \n",
    "print('Sigmoid units RMSEs on validation set:', errs_sig_print)\n",
    "print('Softmax units RMSEs on validation set:', errs_sof_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focus on around 50 to search for the best K\n",
    "# alpha = 30\n",
    "# val_errs = []\n",
    "# for K in range(45, 65, 1):   \n",
    "#     X_train_sof, X_val_sof, _ = make_softmax_units(K)\n",
    "#     train_errs_sof, val_errs_sof = train_small_nn(X_train_sof, X_val_sof, alpha)\n",
    "#     val_errs.append([K, np.round(val_errs_sof, 5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update K with GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nn_softmax_reg(X_train, y_train, test_input, test_output, K, alpha):     \n",
    "    X_train_trans, X_val_trans = make_softmax_units(K)\n",
    "    train_errs, val_errs = train_nn(X_train_trans, X_val_trans, init_a, alpha)    \n",
    "    return val_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "alpha = 0.0001\n",
    "# validation RMSE from Q6 using softmax units net\n",
    "rmse_baseline = 0.2102293480091851\n",
    "\n",
    "# get three observations from training the neural network at three alpha values\n",
    "k_obs = np.array([25, 30, 35])\n",
    "\n",
    "rmse_val = []\n",
    "for K in k_obs:\n",
    "    #val_err = train_nn_reg(X_train, y_train, X_val, y_val, a)\n",
    "    val_err = train_nn_softmax_reg(X_train, y_train, X_val, y_val, K, alpha)\n",
    "    rmse_val.append(val_err)\n",
    "    \n",
    "# subtract your alpha-observed log RMSEs from the log of this baseline and take the resulting values as y^(1), ...,  y^(3)\n",
    "y_obs = np.log(rmse_baseline) - np.log(rmse_val)\n",
    "\n",
    "k_rest = np.arange(2, 64, 1)\n",
    "\n",
    "# remove already observed values of alpha\n",
    "k_rest = np.setdiff1d(k_rest, k_obs)\n",
    "\n",
    "results = []\n",
    "for i in range(5):\n",
    "    # find alpha with highest PI\n",
    "    k_best, max_pi = maximise_acquisition_function(k_rest, k_obs, y_obs)\n",
    "    #print(alpha_best)\n",
    "    \n",
    "    # add to observed locations and remove from test locations\n",
    "    k_obs = np.append(k_obs, k_best)\n",
    "    k_rest = np.setdiff1d(k_rest, k_obs)\n",
    "    \n",
    "    # train using this alpha\n",
    "#     val_rmse_alpha = train_nn_reg(X_train, y_train, X_val, y_val, alpha_best)\n",
    "    val_rmse_k = train_nn_softmax_reg(X_train, y_train, X_val, y_val, k_best, alpha)\n",
    "    y_k = np.log(rmse_baseline) - np.log(val_rmse_k)\n",
    "    \n",
    "    # add to observed values\n",
    "    y_obs = np.append(y_obs, y_k)\n",
    "    \n",
    "    results.append((k_best, max_pi, val_rmse_k))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION: 1, K: 63, PI: 0.8747222916981918, VAL_RMSE: 0.24038823233935383\n",
      "ITERATION: 2, K: 2, PI: 0.8747197928491879, VAL_RMSE: 0.5530904576893079\n",
      "ITERATION: 3, K: 49, PI: 0.8670397889152548, VAL_RMSE: 0.22941343819267312\n",
      "ITERATION: 4, K: 17, PI: 0.7431601431238777, VAL_RMSE: 0.2773592566033777\n",
      "ITERATION: 5, K: 44, PI: 0.648029614599473, VAL_RMSE: 0.22955263112545585\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(results)):\n",
    "    result = results[i]\n",
    "    print(\"ITERATION: {}, K: {}, PI: {}, VAL_RMSE: {}\".format(i+1, result[0], result[1], result[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
